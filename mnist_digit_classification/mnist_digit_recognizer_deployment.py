# -*- coding: utf-8 -*-
"""MNIST digit recognizer_deployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k_EiGZSWVnhzqUrhX0SWmpZF3wS73aA8

# **MNIST digit classifier**

importing libraries
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

"""setting max rows and max columns in pandas"""

pd.set_option('display.max_rows',100)
pd.set_option('display.max_columns',100)

"""importing dataset"""

dataset=pd.read_csv('/content/drive/My Drive/Kaggle/MNIST digit recognizer/train.csv')
test=pd.read_csv('/content/drive/My Drive/Kaggle/MNIST digit recognizer/test.csv')
dataset.head(10)

dataset.info()

"""making into x and y"""

x_train=dataset.loc[:,'pixel0':]
y_train=dataset.loc[:,'label']
x_test=test.loc[:,:]

y_train=pd.get_dummies(y_train)

x_train=np.array(x_train)
y_train=np.array(y_train)
x_test=np.array(x_test)

"""Scaling the dataset"""

x_train=x_train/255.0
x_test=x_test/255.0

"""Reshaping the dataset"""

x_train=x_train.reshape(x_train.shape[0],28,28,1)
x_test=x_test.reshape(x_test.shape[0],28,28,1)

"""Building CNN"""

import tensorflow.keras as tf
model=tf.models.Sequential()

"""1st convolution layer"""

model.add(tf.layers.Conv2D(filters=50,kernel_size=4,padding='same',activation='relu',input_shape=[28,28,1]))
model.add(tf.layers.MaxPool2D(pool_size=2,strides=2,padding='valid'))
model.add(tf.layers.BatchNormalization())
model.add(tf.layers.Dropout(0.25))

"""2nd convolution layer"""

model.add(tf.layers.Conv2D(filters=50,kernel_size=4,padding='same',activation='relu'))
model.add(tf.layers.MaxPool2D(pool_size=2,strides=2,padding='valid'))
model.add(tf.layers.BatchNormalization())
model.add(tf.layers.Dropout(0.25))

"""3rd convolution layer"""

model.add(tf.layers.Conv2D(filters=50,kernel_size=4,padding='same',activation='relu'))
model.add(tf.layers.MaxPool2D(pool_size=2,strides=2,padding='valid'))
model.add(tf.layers.BatchNormalization())
model.add(tf.layers.Dropout(0.25))

"""flattening"""

model.add(tf.layers.Flatten())

"""full connection 1st layer"""

model.add(tf.layers.Dense(units=1024,activation='relu'))
model.add(tf.layers.Dropout(0.25))

"""2nd layer"""

model.add(tf.layers.Dense(units=1024,activation='relu'))
model.add(tf.layers.Dropout(0.25))

"""output layer"""

model.add(tf.layers.Dense(units=10,activation='softmax'))

"""compiling the model"""

model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])

"""training model"""

model.fit(x=x_train,y=y_train,batch_size=50,epochs=10,verbose=1)

y_pred=model.predict(x_test)

print(y_pred)

predictions=[]
for i in y_pred:
    predictions.append(np.argmax(i))
print(predictions)

score = model.evaluate(x_test, y_pred, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])


#Save the model
# serialize model to JSON
model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model.h5")
print("Saved model to disk")